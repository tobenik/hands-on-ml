{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement batch gradient descent with early stopping for softmax regression\n",
    "# - softmax for prediction\n",
    "# - cross entropy as cost function\n",
    "# - gradient of cross entropy as direction\n",
    "# - parameter: no. of epochs since last minimum\n",
    "# - parameter: learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# One hot encoded targets\n",
    "target_onehot = np.zeros((data.target.size, data.target.max()+1), dtype=int)\n",
    "target_onehot[np.arange(data.target.size), data.target] = 1\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, target_onehot, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.25945394e-02, 8.25945394e-02, 2.24515236e-01, 6.10295685e-01],\n",
       "       [3.50964520e-02, 6.42813940e-04, 2.59329652e-01, 7.04931082e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Softmax\n",
    " \n",
    "# softmax score (these go through softmax to get the probs), \n",
    "# theta is the parameter matrix (k, n)\n",
    "# X is the features (m, n)\n",
    "# --> softmax scores for each sample (m, k)\n",
    "def softmax_score(X, theta):\n",
    "    return X @ np.transpose(theta)\n",
    "\n",
    "# turn softmax scores into probabilities\n",
    "# scores from softmax_scores() (m, k)\n",
    "# --> probabilities for each sample (m, k)\n",
    "def softmax(scores):\n",
    "    nominator = np.exp(scores)\n",
    "    denominator = np.sum(nominator, axis=1)[:, np.newaxis]\n",
    "    return nominator / denominator\n",
    "\n",
    "\n",
    "X = np.array([[2,2,3,4], [5,1,7,8]]).reshape(2,4)\n",
    "theta_k = np.array([1,1,0,1]).reshape(-1,1)\n",
    "\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2975,  0.45  ,  0.3   ],\n",
       "       [-0.38  , -0.615 , -0.4075],\n",
       "       [ 0.0825,  0.165 ,  0.1075]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross entropy for batch (ie. cost function)\n",
    "# p probabilities from softmax (m, k)\n",
    "# y targets (m, k)\n",
    "# --> cross entropy (scalar)\n",
    "def cross_entropy(p, y):\n",
    "    cost = -np.mean(np.transpose(np.log(p)) @ y)\n",
    "    return cost\n",
    "\n",
    "# cross entropy gradient matrix\n",
    "# p probabilities from softmax (m, k)\n",
    "# y targets (m, k)\n",
    "# X features (m, n)\n",
    "# --> gradient vector (k, n)\n",
    "def cross_entropy_gradient(p, y, X):\n",
    "    k = y.shape[1]\n",
    "    m, n = X.shape\n",
    "    gradient = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        gradient[i,:] = np.mean((p[:,i] - y[:,i])[:, np.newaxis] * X, axis=0)\n",
    "    return gradient\n",
    "\n",
    "# Features (X) of shape (m, n)\n",
    "X = np.array([[0.5, 1.2, 0.8],\n",
    "              [1.5, 2.3, 1.8],\n",
    "              [0.3, 1.1, 0.7],\n",
    "              [1.0, 2.0, 1.0]])\n",
    "\n",
    "# Predicted probabilities from softmax (p) of shape (m, k)\n",
    "p = np.array([[0.7, 0.2, 0.1],\n",
    "              [0.6, 0.3, 0.1],\n",
    "              [0.8, 0.1, 0.1],\n",
    "              [0.5, 0.4, 0.1]])\n",
    "\n",
    "# One-hot encoded true labels (y) of shape (m, k)\n",
    "y = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [1, 0, 0],\n",
    "              [0, 1, 0]])\n",
    "\n",
    "cross_entropy_gradient(p, y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessing = Pipeline([(\"scale\", StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train_preprocessed = preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.44%\n",
      "Test Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "a = 1 # learning rate\n",
    "threshold = 10\n",
    "theta = np.random.random((y_train.shape[1], X_train_preprocessed.shape[1]))\n",
    "min_cost = float('inf')\n",
    "min_cost_epoch = 0\n",
    "min_cost_theta = theta.copy()\n",
    "cost_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    scores = softmax_score(X_train_preprocessed, theta)\n",
    "    probs = softmax(scores)\n",
    "    cost = cross_entropy(probs, y_train)\n",
    "    cost_history.append(cost)\n",
    "    if cost < min_cost:\n",
    "        min_cost_epoch = e\n",
    "        min_cost = cost\n",
    "        min_cost_theta = theta\n",
    "\n",
    "    if e - min_cost_epoch >= threshold:\n",
    "        print(cost)\n",
    "        print(f\"breaking at epoch: {e, min_cost_epoch}\")\n",
    "        break\n",
    "    else:\n",
    "        theta -= a * cross_entropy_gradient(probs, y_train, X_train_preprocessed)\n",
    "\n",
    "# Final weights after training\n",
    "theta = min_cost_theta\n",
    "\n",
    "# Accuracy on training set\n",
    "train_scores = softmax_score(X_train_preprocessed, theta)\n",
    "train_probs = softmax(train_scores)\n",
    "train_preds = np.argmax(train_probs, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "accuracy = np.mean(train_preds == y_train_labels)\n",
    "print(f\"Train Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Testing on test set\n",
    "test_scores = softmax_score(X_test, theta)\n",
    "test_probs = softmax(test_scores)\n",
    "test_predictions = np.argmax(test_probs, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(test_predictions == y_test_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cost = 0.6884630265432256\n",
      "Epoch 100: Cost = 0.533918007268345\n",
      "Epoch 200: Cost = 0.4766152401901341\n",
      "Epoch 300: Cost = 0.4468655680295043\n",
      "Epoch 400: Cost = 0.4281875941826558\n",
      "Epoch 500: Cost = 0.4150928121699509\n",
      "Epoch 600: Cost = 0.40526130506428687\n",
      "Epoch 700: Cost = 0.3975373827332128\n",
      "Epoch 800: Cost = 0.39127325654512013\n",
      "Epoch 900: Cost = 0.38607261777292406\n",
      "Test Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# One-hot encode the target\n",
    "target_onehot = np.zeros((y.size, y.max() + 1))\n",
    "target_onehot[np.arange(y.size), y] = 1\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# Cross entropy loss\n",
    "def cross_entropy(p, y):\n",
    "    return -np.mean(np.sum(y * np.log(p + 1e-8), axis=1))  # Adding small value for stability\n",
    "\n",
    "# Gradient of the cross-entropy loss\n",
    "def cross_entropy_gradient(p, y, X):\n",
    "    return np.dot((p - y).T, X) / X.shape[0]\n",
    "\n",
    "# Softmax scores\n",
    "def softmax_score(X, theta):\n",
    "    return np.dot(X, theta.T)\n",
    "\n",
    "# Parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "early_stop_threshold = 20\n",
    "num_classes = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Initialize weights\n",
    "theta = np.random.randn(num_classes, num_features)\n",
    "\n",
    "# Early stopping variables\n",
    "min_cost = float('inf')\n",
    "min_cost_epoch = 0\n",
    "min_cost_theta = theta.copy()\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for e in range(epochs):\n",
    "    # Compute softmax probabilities\n",
    "    scores = softmax_score(X_train, theta)\n",
    "    probs = softmax(scores)\n",
    "    \n",
    "    # Compute cost\n",
    "    cost = cross_entropy(probs, y_train)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if cost < min_cost:\n",
    "        min_cost = cost\n",
    "        min_cost_epoch = e\n",
    "        min_cost_theta = theta.copy()  # Save best parameters\n",
    "    \n",
    "    if e - min_cost_epoch >= early_stop_threshold:\n",
    "        print(f\"Early stopping at epoch {e} (no improvement since epoch {min_cost_epoch})\")\n",
    "        break\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    gradient = cross_entropy_gradient(probs, y_train, X_train)\n",
    "    theta -= learning_rate * gradient\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print(f\"Epoch {e}: Cost = {cost}\")\n",
    "\n",
    "# Final weights after training\n",
    "theta = min_cost_theta\n",
    "\n",
    "# Testing on test set\n",
    "test_scores = softmax_score(X_test, theta)\n",
    "test_probs = softmax(test_scores)\n",
    "test_predictions = np.argmax(test_probs, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(test_predictions == y_test_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
